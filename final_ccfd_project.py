# -*- coding: utf-8 -*-
"""Final_ccfd_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Antu129/Credit-Card-Fraud-Detection/blob/main/credit_card_fraud_detection.ipynb
"""

# importing Dependencies
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import recall_score, accuracy_score, f1_score, precision_score, classification_report
import matplotlib.pyplot as plt

# Loading dataset into pandas dataframe
credit_card = pd.read_csv('/content/creditcard.csv')

# first five rows of the dataset
credit_card.head()

# checking the shape of the dataset
credit_card.shape

# checking the dataset information 
credit_card.info()

# checking for null_values
credit_card.isnull().sum()

# dropping the rows containging null values as we have a big dataset to work with
credit_card_data = credit_card.dropna(how='any')

# droping the Time column
credit_card_data = credit_card_data.drop(columns="Time", axis=1)

# checking the shape of the new dataset after droping the nullvalues
credit_card_data.shape

# Analysis of the dataset

# Distribution of the dataset
credit_card_data.hist(figsize= (20,20))
plt.show()

# distribution of legit transactions and fradulent transactions
credit_card_data['Class'].value_counts()

# as the dataset is highly imbalanced, we are gonna seperate the dataset by class
# class = 0 ---> normal tranasactions
# class = 1 ---> fradulent transactions
legit = credit_card_data[credit_card_data.Class==0]
fraud = credit_card_data[credit_card_data.Class==1]

# checking the shape of the two seperated datasets
print('Legit :',legit.shape)
print('Fraud :',fraud.shape)

# statistical information of the datasets

# legit transactions
legit.Amount.describe()

# fraud transactions
fraud.Amount.describe()

# comparing the mean values of the whole dataset separeted by class
credit_card_data.groupby('Class').mean()

# Sampling
# we have used undersampling method for the sample as the dataset is highly imbalanced
# we took a sample from the legit dataset randomly. And number of the sample data is same as the fraud dataset
num_of_fraud_transaction = fraud.value_counts().sum()
legit_sample = legit.sample(n=num_of_fraud_transaction, random_state = 3)

# making a sample of the dataset for the model by concatennating legit_sample and fraud transactions
new_dataset = pd.concat([legit_sample,fraud],axis=0)

# shape of the sample dataset
new_dataset.shape

# number of legit and fraudulent transactions in the sample
new_dataset["Class"].value_counts()

# checking the mean of the sample dataset seperated by class 
new_dataset.groupby('Class').mean()

# spliting the dataset into features and targets
# X ---> features/input
# Y ---> target/output
X = new_dataset.drop(columns="Class", axis=1)
Y = new_dataset['Class']
print('Features: \n\n', X)
print('Target: \n\n', Y)

# spliting the dataset into training data and testing data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y, random_state = 3)
print(X.shape,X_train.shape,X_test.shape)

# Model training
# we have used Logistic regression technique 
model = LogisticRegression(max_iter = 1000)
# tarining the model 
model.fit(X_train, Y_train)

# model interpretation using cross validation
# show the mean of the scores of cross validation
# show the variance of the scores
scores = cross_val_score(model, X, Y, cv=10)
mean_cross_val = scores.mean()
print("Mean : ",mean_cross_val)
vari_cross_val = scores.std()
print("Variance: ", vari_cross_val)

# Evaluation
# accuracy score on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
print("Accuracy on training data: ", training_data_accuracy)

# accuracy score on testing data
X_test_prediction = model.predict(X_test)
testing_data_accuracy = accuracy_score(X_test_prediction, Y_test)
print("Accuracy on training data: ", testing_data_accuracy)

# precision, recall and f1-score report of the testing data
print(classification_report(Y_test,X_test_prediction))